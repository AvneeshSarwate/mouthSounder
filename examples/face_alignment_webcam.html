<!doctype html>
<html>
<head>
        <meta charset="utf-8">
        <title>tracking.js - face alignment with camera</title>
        <!-- here is the frame around each example - to be removed - to a fullscreen video - working on mobile too -->
        <link rel="stylesheet" href="assets/demo.css">
        
        <script src="../build/tracking.js"></script>
        <script src="../build/data/face-min.js"></script>
        <script src="../src/alignment/training/Landmarks.js"></script>
        <script src="../src/alignment/training/Regressor.js"></script>
        <script src="../src/mouthFiles/audioLoad.js"></script>
        <script
  src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
  integrity="sha256-3edrmyuQ0w65f8gfBsqowzjJe2iM6n0nKciPUp8y+7E="
  crossorigin="anonymous"></script>
        
        <script src="../node_modules/dat.gui/build/dat.gui.min.js"></script>
        <script src="../node_modules/tone/build/Tone.js"></script>
        <script src="./assets/nixon_transcript.js"></script>
</head>
<body>
        <style>
                #videoWebcam {
                        position: absolute;
                        top: 0px;
                        left: 0px;
                        width : 320px;
                        height: auto;
                        zoom: 3;
                }
                #canvasDetection {
                        position: absolute;
                        top: 0px;
                        left: 0px;
                        width : 320px;
                        height: auto;
                        zoom: 3;
                }
                #buttons {
                        position: absolute;
                        top: 240px;
                        left: 0px;
                        width : 320px;
                        height: auto;
                        zoom: 3;
                }
        </style>
	<!-- <video id="videoWebcam" width="368" height="288" autoplay loop>
		<source src="./assets/franck.mp4" type="video/mp4"/>
		<source src="./assets/franck.ogv" type="video/ogg"/>
	</video> -->
        <div>
                <video id="videoWebcam" preload autoplay loop muted></video>
                <canvas id="canvasDetection"></canvas>
        </div>
        <div id="buttons">
                <button id="mouthOpen">Record mouth open</button>
                <button id="mouthClosed">Record mouth cloused</button>   
                <div id="distText">
                        
                </div>
                <div id="calibrationStatus">
                        
                </div>
                <div id="triggerLog">
                        
                </div>
        </div>
        <audio id="clipAudio" src="./assets/nixon_win.mp3"></audio>
        
        
<script>
        var canvasDetection = document.querySelector('#canvasDetection');
        canvasDetection.width = 320
        canvasDetection.height = 240
        var context = canvasDetection.getContext('2d');

        // tracking.LBF.maxNumStages = 10
        var tracker = new tracking.LandmarksTracker();
        tracker.setEdgesDensity(0.1);
        tracker.setInitialScale(4);
        tracker.setStepSize(2);

        tracker.setInitialScale(2);
        tracker.setStepSize(1);

        
        var gui = new dat.GUI();
        gui.add(tracker, 'edgesDensity', 0.1, 0.5).step(0.01).listen();
        gui.add(tracker, 'initialScale', 1.0, 10.0).step(0.1).listen();
        gui.add(tracker, 'stepSize', 0.5, 5).step(0.1).listen();
        

        var wordlist = nixonTranscript.results.map(r => r.alternatives[0].words).flat(1)
            .map(w => ({word:w.word, start: parseFloat(w.startTime.slice(0, -1)), end: parseFloat(w.endTime.slice(0, -1))}));
        var audioSource = $("#clipAudio")[0];

        var videoElement = document.querySelector('#videoWebcam')
        // tracking.track(videoElement, tracker);
        tracking.track(videoElement, tracker, { camera: true });
        
        var landmarksPerFace = 30
        var landmarkFeatures = {
                jaw : {
                        first: 0,
                        last: 8,
                        fillStyle: 'white',
                        closed: false,
                },
                nose : {
                        first:15,
                        last: 18,
                        fillStyle: 'green',
                        closed: true,
                },
                mouth : {
                        first:27,
                        last: 30,
                        fillStyle: 'red',
                        closed: true,
                },
                eyeL : {
                        first:19,
                        last: 22,
                        fillStyle: 'purple',
                        closed: false,
                },
                eyeR : {
                        first:23,
                        last: 26,
                        fillStyle: 'purple',
                        closed: false,
                },
                eyeBrowL : {
                        first: 9,
                        last: 11,
                        fillStyle: 'yellow',
                        closed: false,
                },
                eyeBrowR : {
                        first:12,
                        last: 14,
                        fillStyle: 'yellow',
                        closed: false,
                },
        }

        //////////////////////////////////////////////////////////////////////////////
        //                Code Separator
        //////////////////////////////////////////////////////////////////////////////
        var parameters = {
                landmarkLerpFactor : 0.7,
                boundinBoxVisible : true,
                jawVisible : true,
                eyeBrowLVisible : true,
                eyeBrowRVisible : true,
                noseVisible : true,
                eyeLVisible : true,
                eyeRVisible : true,
                mouthVisible : true,
        }
        gui.add(parameters, 'landmarkLerpFactor', 0.0, 1).listen().name('Landmarks Lerp');
        gui.add(parameters, 'boundinBoxVisible', 0.0, 1).listen().name('bounding box');
        Object.keys(landmarkFeatures).forEach(function(featureLabel){
                gui.add(parameters, featureLabel + 'Visible').listen().name(featureLabel);
        })

        var lerpedFacesLandmarks = []
        
        function chinToEyesDistance(faceLandmarks){
                var chinIdx = [3, 4, 5];
                var eyesIdx = [19, 20, 21, 22, 23, 24, 25, 26];
                var totalDist = 0;
                chinIdx.forEach(function(cIdx){
                        eyesIdx.forEach(function(eIdx){
                                var chinPt = faceLandmarks[cIdx];
                                var eyePt = faceLandmarks[eIdx];
                                totalDist += ((chinPt[0]-eyePt[0])**2 + (chinPt[1]-eyePt[1])**2)**0.5;
                        });
                });
                return totalDist;
        }

        var distanceBuffer = new Array(10).fill(0);
        var calibrationBuffer = new Array(50).fill(0);
        var calibrationInd = 0;
        var calibrationStatus = ""; //is either "open", "closed", or falsy 
        var faceStates = {open: 0, closed: 0};
        var faceThreshold = 0; //set this to difference between states/4
        var lastDefiniteState = ""; //will only be "open" or "closed" - used to detect open/close transition even though "neither" statuses will be caused by noise

        var distanceBufferInd = 0;
        var average = (array) => array.reduce((a, b) => a + b) / array.length;

        $("#mouthOpen").click(function(){
                calibrationStatus = "open";
                $("#calibrationStatus").html("Recording open mouth state");
        });

        $("#mouthClosed").click(function(){
                calibrationStatus = "closed";
                $("#calibrationStatus").html("Recording closed mouth state");
        });

        tracker.on('track', function(event) {
                // clear debug canvasDetection
                context.clearRect(0,0,canvasDetection.width, canvasDetection.height);

                if( event.data === undefined ) return;
                
                event.data.faces.forEach(function(boundingBox, faceIndex) {
                        var faceLandmarks = event.data.landmarks[faceIndex]

                        if( parameters.boundinBoxVisible === true ) displayFaceLandmarksBoundingBox(boundingBox, faceIndex)

                        // lerpFacesLandmarks
                        lerpFacesLandmarks(faceLandmarks)
                        
                        // display each faceLandmarks
                        displayFaceLandmarksDot(lerpedFacesLandmarks)
                        
                        calculateMouthState(faceLandmarks);
                        
                });
        })

        var randNotes = ["C3", "D#3", "F#3", "A3"];
        var sampler = new Tone.Sampler({
            "D3" : "./assets/wow.mp3",
        }).toMaster();

        function calculateMouthState(faceLandmarks){
                var totalDist = chinToEyesDistance(faceLandmarks);
                distanceBuffer[distanceBufferInd] = totalDist;
                distanceBufferInd = (distanceBufferInd + 1) % distanceBuffer.length;
                var distAvg = average(distanceBuffer).toFixed(2);

                if(calibrationStatus){
                        if(calibrationInd == calibrationBuffer.length){
                                faceStates[calibrationStatus] = average(calibrationBuffer);
                                calibrationStatus = "";
                                calibrationInd = 0;
                                if(faceStates.open > 0 && faceStates.closed > 0) faceThreshold = Math.abs(faceStates.open - faceStates.closed)/4;
                                $("#calibrationStatus").html("");
                        } else {
                                calibrationBuffer[calibrationInd] = totalDist;
                                calibrationInd++;
                        }
                }

                if(faceThreshold > 0) {
                        var mouthState = "neither";
                        if(Math.abs(faceStates.open-distAvg) < faceThreshold) mouthState = "open";
                        else if(Math.abs(faceStates.closed-distAvg) < faceThreshold) mouthState = "closed";
                        $("#calibrationStatus").html(mouthState);
                        if(mouthState === "open" && lastDefiniteState === "closed"){
                                //trigger mouth opened event
                                $("#triggerLog").html("ON");
                                // sampler.triggerAttack("D3");
                                var wordInd = Math.floor(Math.random() * wordlist.length);
                                audioSource.currentTime = wordlist[wordInd].start;
                                audioSource.play();
                                console.log("playing at", audioSource.currentTime);
                        }

                        else if(mouthState === "closed" && lastDefiniteState === "open"){
                                //trigger mouth closed event
                                $("#triggerLog").html("OFF");
                                audioSource.pause();
                                console.log("pause at", audioSource.currentTime);
                        } else $("#triggerLog").html("");

                        if(mouthState != "neither") lastDefiniteState = mouthState;
                }

                if(distanceBufferInd == 0) $("#distText").html(`dist: ${distAvg} <br> closed: ${faceStates.closed.toFixed(2)} open: closed: ${faceStates.open.toFixed(2)}`);
        }

        function lerpFacesLandmarks(newFaceLandmarks){
                // init lerpFacesLandmarks if needed
                for(var i = 0; i < newFaceLandmarks.length; i++){
                        if( lerpedFacesLandmarks[i] !== undefined ) continue
                        lerpedFacesLandmarks[i] = [
                                newFaceLandmarks[i][0],
                                newFaceLandmarks[i][1],
                        ]                        
                }

                // init lerpFacesLandmarks if needed
                for(var i = 0; i < newFaceLandmarks.length; i++){
                        var lerpFactor = parameters.landmarkLerpFactor
                        lerpedFacesLandmarks[i][0] = newFaceLandmarks[i][0] * lerpFactor  + lerpedFacesLandmarks[i][0] * (1-lerpFactor)
                        lerpedFacesLandmarks[i][1] = newFaceLandmarks[i][1] * lerpFactor  + lerpedFacesLandmarks[i][1] * (1-lerpFactor)
                }
        }

        //////////////////////////////////////////////////////////////////////////////
        //                Code Separator
        //////////////////////////////////////////////////////////////////////////////
        function displayFaceLandmarksBoundingBox(boundingBox, faceIndex){
                // display the box
                context.strokeStyle = '#a64ceb';
                context.strokeRect(boundingBox.x, boundingBox.y, boundingBox.width, boundingBox.height);

                // display the size of the box
                context.font = '11px Helvetica';
                context.fillStyle = "#fff";
                context.fillText('idx: '+faceIndex, boundingBox.x + boundingBox.width + 5, boundingBox.y + 11);
                context.fillText('x: ' + boundingBox.x + 'px', boundingBox.x + boundingBox.width + 5, boundingBox.y + 22);
                context.fillText('y: ' + boundingBox.y + 'px', boundingBox.x + boundingBox.width + 5, boundingBox.y + 33);
        }
        
        function displayFaceLandmarksDot(faceLandmarks){
                Object.keys(landmarkFeatures).forEach(function(featureLabel){
                        if( parameters[featureLabel+'Visible'] === false )      return
                        displayFaceLandmarksFeature(faceLandmarks, featureLabel)
                })
        }
        function displayFaceLandmarksFeature(faceLandmarks, featureLabel){
                var feature = landmarkFeatures[featureLabel]
                
                // draw dots
                context.fillStyle = feature.fillStyle
                for(var i = feature.first; i <= feature.last; i++){
                        var xy = faceLandmarks[i]
                        context.beginPath();
                        context.arc(xy[0],xy[1],1,0,2*Math.PI);
                        context.fill();                                
                }                
                
                // draw lines
                var feature = landmarkFeatures[featureLabel]
                context.strokeStyle = feature.fillStyle
                context.beginPath();
                for(var i = feature.first; i <= feature.last; i++){
                        var x = faceLandmarks[i][0]
                        var y = faceLandmarks[i][1]
                        if( i === 0 ){
                                context.moveTo(x, y)
                        }else{
                                context.lineTo(x, y)
                        }
                }                
                if( feature.closed === true ){
                        var x = faceLandmarks[feature.first][0]
                        var y = faceLandmarks[feature.first][1]
                        context.lineTo(x, y)
                }

                context.stroke();

        }
</script></body>
